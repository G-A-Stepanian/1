# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
"""

import requests
import time
import re
from bs4 import BeautifulSoup
from config import SEARCH_DELAY, MAX_RETRIES, TIMEOUT


class WebSearcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

    def search_google(self, query):
        """
        –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (–∏—Å–ø–æ–ª—å–∑—É—è requests)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            url = f"https://www.google.com/search?q={requests.utils.quote(query)}"
            response = self.session.get(url, timeout=TIMEOUT)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []

                # –ò—â–µ–º –±–ª–æ–∫–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                for g in soup.find_all('div', class_='g')[:5]:  # –ü–µ—Ä–≤—ã–µ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    title_elem = g.find('h3')
                    link_elem = g.find('a')
                    snippet_elem = g.find('div', class_=['VwiC3b', 'yXK7lf'])

                    if title_elem and link_elem:
                        results.append({
                            'title': title_elem.get_text(),
                            'url': link_elem.get('href'),
                            'snippet': snippet_elem.get_text() if snippet_elem else ''
                        })

                return results

        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Google: {e}")

        return []

    def search_duckduckgo(self, query):
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo
        """
        try:
            url = f"https://html.duckduckgo.com/html/?q={requests.utils.quote(query)}"
            response = self.session.get(url, timeout=TIMEOUT)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []

                for result in soup.find_all('div', class_='result')[:5]:
                    title_elem = result.find('a', class_='result__a')
                    snippet_elem = result.find('a', class_='result__snippet')

                    if title_elem:
                        results.append({
                            'title': title_elem.get_text(),
                            'url': title_elem.get('href'),
                            'snippet': snippet_elem.get_text() if snippet_elem else ''
                        })

                return results

        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ DuckDuckGo: {e}")

        return []

    def find_info(self, country, question):
        """
        –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É –¥–ª—è —Å—Ç—Ä–∞–Ω—ã
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        query = f"{country} {question} 2026"

        print(f"üîç –ò—â—É: {query}")

        # –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google
        results = self.search_google(query)

        # –ï—Å–ª–∏ Google –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º DuckDuckGo
        if not results:
            time.sleep(SEARCH_DELAY)
            results = self.search_duckduckgo(query)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if results:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            answer = self._extract_answer(results, question)
            time.sleep(SEARCH_DELAY)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            return answer

        return "–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

    def _extract_answer(self, results, question):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π snippet —Å —Å—Å—ã–ª–∫–æ–π
        """
        if results and results[0].get('snippet'):
            snippet = results[0]['snippet']
            url = results[0].get('url', '')

            # –û—á–∏—â–∞–µ–º snippet –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            snippet = snippet.strip()

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
            if len(snippet) > 200:
                snippet = snippet[:200] + "..."

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º snippet + –∏—Å—Ç–æ—á–Ω–∏–∫
            return f"{snippet}\n[–ò—Å—Ç–æ—á–Ω–∏–∫: {url}]"

        return "–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
if __name__ == "__main__":
    searcher = WebSearcher()
    result = searcher.find_info("–ß–µ—Ä–Ω–æ–≥–æ—Ä–∏—è", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –∞—Ä–µ–Ω–¥—ã –∂–∏–ª—å—è –Ω–∞ —Å–µ–º—å—é –∏–∑ 4 —á–µ–ª–æ–≤–µ–∫")
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{result}")
